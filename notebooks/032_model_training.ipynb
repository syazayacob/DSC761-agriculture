{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7663f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318b4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN: TensorFlow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91d59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create directories ===\n",
    "os.makedirs(\"../models/improve\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2923b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load cleaned & log-transformed data ===\n",
    "#df = pd.read_csv(\"../data/improve/crop_data_pivot_log.csv\")\n",
    "\n",
    "url = \"https://huggingface.co/datasets/syazayacob/crop_data_pivot_log/resolve/main/crop_data_pivot_log.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1c067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define target variables ===\n",
    "targets = [\"Production\", \"Area harvested\", \"Yield\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4079925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define ANN training function ===\n",
    "def train_ann(X_train, y_train, X_test, y_test, model_path):\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='relu')(input_layer)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss=MeanSquaredError())\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_split=0.2,\n",
    "              epochs=50,\n",
    "              batch_size=32,\n",
    "              callbacks=[es],\n",
    "              verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    model.save(f\"{model_path}.h5\")\n",
    "    return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262b1831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Training models to predict: Production\n",
      "🔹 Training ANN...\n",
      "\u001b[1m3599/3599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 489us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Random Forest...\n",
      "🔹 Training Linear Regression...\n",
      "🔹 Training XGBoost...\n",
      "\n",
      "📌 Training models to predict: Area harvested\n",
      "🔹 Training ANN...\n",
      "\u001b[1m3599/3599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Random Forest...\n",
      "🔹 Training Linear Regression...\n",
      "🔹 Training XGBoost...\n",
      "\n",
      "📌 Training models to predict: Yield\n",
      "🔹 Training ANN...\n",
      "\u001b[1m3599/3599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Random Forest...\n",
      "🔹 Training Linear Regression...\n",
      "🔹 Training XGBoost...\n",
      "\n",
      "✅ All models trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# === Training loop for each target ===\n",
    "results = []\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\n📌 Training models to predict: {target}\")\n",
    "\n",
    "    # Features = other targets + 'Year'\n",
    "    feature_cols = [col for col in targets if col != target] + [\"Year\"]\n",
    "\n",
    "    # Drop rows with missing target/features\n",
    "    data = df.dropna(subset=[target] + feature_cols).copy()\n",
    "    X = data[feature_cols]\n",
    "    y = data[target]\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # === Train ANN ===\n",
    "    print(\"🔹 Training ANN...\")\n",
    "    y_pred_ann, model_ann = train_ann(X_train, y_train, X_test, y_test, f\"../models/{target}_ANN\")\n",
    "    joblib.dump(scaler, f\"../models/{target}_ANN_scaler.pkl\", compress=3)\n",
    "\n",
    "    mse_ann = mean_squared_error(y_test, y_pred_ann)\n",
    "    r2_ann = r2_score(y_test, y_pred_ann)\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"ANN\", \"MSE\": mse_ann, \"R2\": r2_ann})\n",
    "\n",
    "    # === Train Random Forest ===\n",
    "    print(\"🔹 Training Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    joblib.dump({\"model\": rf, \"scaler\": scaler}, f\"../models/{target}_RandomForest.pkl\", compress=3)\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"RandomForest\", \"MSE\": mean_squared_error(y_test, y_pred_rf), \"R2\": r2_score(y_test, y_pred_rf)})\n",
    "\n",
    "    # === Train Linear Regression ===\n",
    "    print(\"🔹 Training Linear Regression...\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    joblib.dump({\"model\": lr, \"scaler\": scaler}, f\"../models/{target}_LinearRegression.pkl\")\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"LinearRegression\", \"MSE\": mean_squared_error(y_test, y_pred_lr), \"R2\": r2_score(y_test, y_pred_lr)})\n",
    "\n",
    "    # === Train XGBoost ===\n",
    "    print(\"🔹 Training XGBoost...\")\n",
    "    xg = XGBRegressor(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "    xg.fit(X_train, y_train)\n",
    "    y_pred_xg = xg.predict(X_test)\n",
    "    joblib.dump({\"model\": xg, \"scaler\": scaler}, f\"../models/{target}_XGBoost.pkl\")\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"XGBoost\", \"MSE\": mean_squared_error(y_test, y_pred_xg), \"R2\": r2_score(y_test, y_pred_xg)})\n",
    "\n",
    "print(\"\\n✅ All models trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa580bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Model Evaluation Results:\n",
      "            Target             Model       MSE        R2\n",
      "5   Area harvested      RandomForest  0.011598  0.998755\n",
      "7   Area harvested           XGBoost  0.020844  0.997762\n",
      "6   Area harvested  LinearRegression  0.042174  0.995473\n",
      "4   Area harvested               ANN  0.052867  0.994325\n",
      "1       Production      RandomForest  0.002192  0.999772\n",
      "3       Production           XGBoost  0.010073  0.998951\n",
      "0       Production               ANN  0.040743  0.995756\n",
      "2       Production  LinearRegression  0.041392  0.995688\n",
      "9            Yield      RandomForest  0.020535  0.989483\n",
      "8            Yield               ANN  0.029938  0.984668\n",
      "11           Yield           XGBoost  0.031472  0.983882\n",
      "10           Yield  LinearRegression  0.042631  0.978167\n"
     ]
    }
   ],
   "source": [
    "# === Summary table ===\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=[\"Target\", \"R2\"], ascending=[True, False])\n",
    "print(\"\\n📊 Model Evaluation Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba8ead31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Repository created or already exists: syazayacob/crop_models\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a032c5c9158419ba356eda4c5903e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Area harvested_ANN.h5:   0%|          | 0.00/70.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Area harvested_ANN.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14469f2b92ea4e1e8cc4c33c8698fac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Area harvested_ANN_scaler.pkl:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Area harvested_ANN_scaler.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec74d977499e41f38d23eb952bb60cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Area harvested_LinearRegression.pkl:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Area harvested_LinearRegression.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31ff35ca3584dbf92c7080cf46907fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Area harvested_RandomForest.pkl:   0%|          | 0.00/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Area harvested_RandomForest.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f7c8098a834bb994d8aa0dc7f4526f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Area harvested_XGBoost.pkl:   0%|          | 0.00/254k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Area harvested_XGBoost.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c13c3d73a848ebb2be136e56b08669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production_ANN.h5:   0%|          | 0.00/70.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Production_ANN.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0f5944716a4eda95b128a642851a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production_ANN_scaler.pkl:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Production_ANN_scaler.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effb59b958664f10834ed619bf91028c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production_LinearRegression.pkl:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Production_LinearRegression.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b194b7a59be464fb37bca467a530a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production_RandomForest.pkl:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Production_RandomForest.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15df5acdfc648cb90e82eb1d18f46f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production_XGBoost.pkl:   0%|          | 0.00/254k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Production_XGBoost.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6bf20088d5456d93fbf99b900d86f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Yield_ANN.h5:   0%|          | 0.00/70.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Yield_ANN.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a590ffe4c4b0080e9e9d43429709e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Yield_ANN_scaler.pkl:   0%|          | 0.00/691 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Yield_ANN_scaler.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc999f14c89f450191ed1bb61683cabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Yield_LinearRegression.pkl:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Yield_LinearRegression.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ff2ca01bd94caabd363eb28ca61e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Yield_RandomForest.pkl:   0%|          | 0.00/461M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Yield_RandomForest.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd6075b129f45c9a44cce71d9b2a921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Yield_XGBoost.pkl:   0%|          | 0.00/253k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: Yield_XGBoost.pkl\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, upload_file, create_repo\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "create_repo(\"syazayacob/crop_models\", repo_type=\"model\", token=HF_TOKEN)\n",
    "\n",
    "REPO_ID = \"syazayacob/crop_models\"\n",
    "\n",
    "# Corrected create_repo call\n",
    "api = HfApi()\n",
    "try:\n",
    "    api.create_repo(repo_id=REPO_ID, token=HF_TOKEN, repo_type=\"model\", exist_ok=True)\n",
    "    print(f\"✅ Repository created or already exists: {REPO_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create repo: {e}\")\n",
    "\n",
    "# Adjust this path to match where your files are stored\n",
    "model_folder = \"../models\"\n",
    "\n",
    "model_files = [\n",
    "    \"Area harvested_ANN.h5\",\n",
    "    \"Area harvested_ANN_scaler.pkl\",\n",
    "    \"Area harvested_LinearRegression.pkl\",\n",
    "    \"Area harvested_RandomForest.pkl\",\n",
    "    \"Area harvested_XGBoost.pkl\",\n",
    "    \"Production_ANN.h5\",\n",
    "    \"Production_ANN_scaler.pkl\",\n",
    "    \"Production_LinearRegression.pkl\",\n",
    "    \"Production_RandomForest.pkl\",\n",
    "    \"Production_XGBoost.pkl\",\n",
    "    \"Yield_ANN.h5\",\n",
    "    \"Yield_ANN_scaler.pkl\",\n",
    "    \"Yield_LinearRegression.pkl\",\n",
    "    \"Yield_RandomForest.pkl\",\n",
    "    \"Yield_XGBoost.pkl\"\n",
    "]\n",
    "\n",
    "# Upload with corrected file paths\n",
    "for filename in model_files:\n",
    "    full_path = os.path.join(model_folder, filename)\n",
    "    if os.path.exists(full_path):\n",
    "        try:\n",
    "            upload_file(\n",
    "                path_or_fileobj=full_path,\n",
    "                path_in_repo=filename,\n",
    "                repo_id=REPO_ID,\n",
    "                repo_type=\"model\",\n",
    "                token=HF_TOKEN\n",
    "            )\n",
    "            print(f\"✅ Uploaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to upload {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found: {full_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
